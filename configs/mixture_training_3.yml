model_name: "EleutherAI/gpt-neo-125M"
batch_size: 8
num_epochs: 3
num_experts: 6
learning_rate: 0.0001
device: "cuda"
save_dir: "./checkpoints"
trainer_id: "multiagent-mixer"
lora_r: 8
lora_alpha: 32
lora_dropout: 0.1
selection_temperature: 3.0
early_stopping_patience: 2
cluster_checkpoint_dir: /home/xwang397/scr4_jeisner1/tomwang/multi-agent-finetuning/checkpoints/new-data-split-epoch-15